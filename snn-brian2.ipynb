{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fdfbc9b-307b-460d-9442-3e9b972cb313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brian2 import *\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "120f0059-270a-4160-839e-892bbcb45964",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_seqs = torch.load('raw_train_seqs.pt')\n",
    "raw_test_seqs = torch.load('raw_test_seqs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24938df9-fddd-4e48-b7af-644c1174bade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_encode(tensor, d):\n",
    "\n",
    "    n, features = tensor.shape\n",
    "    assert features == 5, \"The tensor must have 5 features per data point\"\n",
    "    assert d % features == 0, \"d must be divisible by 5.\"\n",
    "\n",
    "    max_firing_rate= d/5\n",
    "    print(max_firing_rate)\n",
    "    time_frame = d // features\n",
    "\n",
    "    normalized_data = (tensor - np.min(tensor)) / (np.max(tensor) - np.min(tensor))\n",
    "\n",
    "    spike_trains = np.zeros((n, d))\n",
    "    for i in range(n):\n",
    "        encoded_data_point = np.array([])\n",
    "        for j in range(features):\n",
    "            # count spikes\n",
    "            spike_count = np.round(normalized_data[i, j] * max_firing_rate).astype(int)\n",
    "            # doesn't exceed time frame\n",
    "            spike_count = min(spike_count, time_frame)\n",
    "            # generate spike train\n",
    "            feature_train = np.zeros(time_frame)\n",
    "            if spike_count > 0:\n",
    "                spike_times = np.random.choice(time_frame, spike_count, replace=False)\n",
    "                feature_train[spike_times] = 1\n",
    "            \n",
    "            encoded_data_point = np.concatenate((encoded_data_point, feature_train))\n",
    "        \n",
    "        spike_trains[i] = encoded_data_point\n",
    "\n",
    "    return spike_trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe9e2975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert spike trains to spike times\n",
    "def convert_to_spike_times(spike_train, dt):\n",
    "    spike_indices = np.where(spike_train == 1)[0]\n",
    "    times = spike_indices * dt\n",
    "    # set indices to 0 as each SpikeGeneratorGroup has only one neuron\n",
    "    indices = np.zeros_like(spike_indices)\n",
    "    return indices, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0ed4659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recorded_spikes_to_dataset(recorded_spikes, num_selected_neurons):\n",
    "    n = num_selected_neurons\n",
    "    indices = []\n",
    "    for i in range(n):\n",
    "        step = 50 // n\n",
    "        indices.append(step * i)\n",
    "    \n",
    "    num_data_points = len(recorded_spikes)\n",
    "    dataset = np.zeros((num_data_points, n))\n",
    "    \n",
    "    for i in range(num_data_points):\n",
    "        spike_data = recorded_spikes[i]\n",
    "        for idx in range(num_selected_neurons): # access the neurons \n",
    "            j = indices[idx]\n",
    "            spikes_j = spike_data[j]\n",
    "            dataset[i, idx] = len(spikes_j)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee57748d-f4f8-434b-80ac-33383425d8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0\n",
      "20.0\n"
     ]
    }
   ],
   "source": [
    "d = 100 \n",
    "# define the length of spike train per data point\n",
    "# d/5 will be the length of spike trains per input neuron\n",
    "\n",
    "train_spikes = rate_encode(raw_train_seqs.numpy(), d)\n",
    "test_spikes = rate_encode(raw_test_seqs.numpy(), d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d2aee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation settings\n",
    "start_scope()\n",
    "num_neurons = 50\n",
    "duration = 1 * second\n",
    "\n",
    "net = Network()\n",
    "\n",
    "# Neuron model\n",
    "eqs = '''\n",
    "dv/dt = (I-v)/tau : 1\n",
    "I : 1\n",
    "tau : second\n",
    "'''\n",
    "\n",
    "# reservoir\n",
    "G = NeuronGroup(num_neurons, eqs, threshold='v>1', reset='v = 0', method='exact')\n",
    "G.v = 'rand()'\n",
    "G.I = 'rand()'\n",
    "G.tau = '2.5*ms'\n",
    "net.add(G)\n",
    "\n",
    "# STDP model\n",
    "tau_pre = tau_post = 5*ms\n",
    "wmax = 0.5\n",
    "A_pre = 0.01\n",
    "A_post = -A_pre * 1.05\n",
    "\n",
    "stdp_eqs = '''\n",
    "w : 1\n",
    "dapre/dt = -apre / tau_pre : 1 (event-driven)\n",
    "dapost/dt = -apost / tau_post : 1 (event-driven)\n",
    "'''\n",
    "\n",
    "# Synapses\n",
    "p_connect = 0.1  # prob of connection\n",
    "S = Synapses(G, G, model=stdp_eqs, on_pre='v_post += w; apre += A_pre; w = clip(w + apost, 0, wmax)',\n",
    "             on_post='apost += A_post; w = clip(w + apre, 0, wmax)')\n",
    "\n",
    "\n",
    "S.connect(p=p_connect)\n",
    "S.w = 'rand() * wmax'  # weight init\n",
    "net.add(S)  \n",
    "\n",
    "# Monitor\n",
    "M = SpikeMonitor(G)\n",
    "net.add(M)\n",
    "\n",
    "# set up input neurons\n",
    "num_input_neurons = 5\n",
    "input_neurons = [SpikeGeneratorGroup(1, [0], [0*ms], dt=None) for _ in range(num_input_neurons)]\n",
    "\n",
    "# connect input neurons to the network\n",
    "for inp_neuron in input_neurons:\n",
    "    S = Synapses(inp_neuron, G, on_pre='v += 0.5')\n",
    "    S.connect(p=0.5)\n",
    "    net.add(inp_neuron)\n",
    "    net.add(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbc21185",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    'tau_post' is an internal variable of group 'synapses', but also exists in the run namespace with the value 5. * msecond. The internal variable will be used. [brian2.groups.group.Group.resolve.resolution_conflict]\n",
      "WARNING    'tau_pre' is an internal variable of group 'synapses', but also exists in the run namespace with the value 5. * msecond. The internal variable will be used. [brian2.groups.group.Group.resolve.resolution_conflict]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n"
     ]
    }
   ],
   "source": [
    "segment_length = len(train_spikes[0]) // 5 \n",
    "print(segment_length)\n",
    "duration_of_data_point = 50*ms\n",
    "\n",
    "recorded_spikes = []\n",
    "\n",
    "idx = 0\n",
    "\n",
    "for data_point in train_spikes:\n",
    "\n",
    "    for i in range(num_input_neurons):\n",
    "        start, end = i * segment_length, (i + 1) * segment_length\n",
    "        segment = data_point[start:end]\n",
    "        indices, times = convert_to_spike_times(segment, 5*ms)\n",
    "#         print(times)\n",
    "        \n",
    "        input_neurons[i].set_spikes(indices, times)\n",
    "    \n",
    "    net.run(duration_of_data_point)\n",
    "    net.t_ = 0*ms\n",
    "    recorded_spikes.append(M.spike_trains())\n",
    "\n",
    "    # clear and reinit\n",
    "    net.remove(M) \n",
    "    M = SpikeMonitor(G) \n",
    "    net.add(M)\n",
    "    \n",
    "    G.v = '0'\n",
    "\n",
    "    for inp_neuron in input_neurons:\n",
    "        inp_neuron.set_spikes([0], [0*ms])\n",
    "        \n",
    "    if idx % 10 == 0:\n",
    "        print(idx)\n",
    "    idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bf34d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597\n"
     ]
    }
   ],
   "source": [
    "print(len(recorded_spikes))\n",
    "n_out = 10\n",
    "x_train_rs = recorded_spikes\n",
    "x_train_snn = recorded_spikes_to_dataset(recorded_spikes, n_out)\n",
    "x_train_snn = torch.from_numpy(x_train_snn)\n",
    "torch.save(x_train_snn, 'x_train_snn.pt')\n",
    "\n",
    "X_train_snn = torch.load('x_train_snn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c3816d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "24\n",
      "18\n",
      "7\n",
      "6\n",
      "114\n",
      "7\n",
      "15\n",
      "13\n",
      "0\n",
      "16\n",
      "104\n",
      "109\n",
      "8\n",
      "106\n",
      "24\n",
      "28\n",
      "119\n",
      "108\n",
      "104\n",
      "8\n",
      "4\n",
      "110\n",
      "111\n",
      "111\n",
      "34\n",
      "5\n",
      "111\n",
      "25\n",
      "0\n",
      "7\n",
      "3\n",
      "114\n",
      "0\n",
      "118\n",
      "13\n",
      "127\n",
      "106\n",
      "7\n",
      "112\n",
      "107\n",
      "115\n",
      "6\n",
      "178\n",
      "0\n",
      "21\n",
      "4\n",
      "121\n",
      "0\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "# print to see spike counts of each neuron for each data point\n",
    "idx = 50\n",
    "spike_data = recorded_spikes[idx]\n",
    "\n",
    "for neuron_index in spike_data:\n",
    "    spikes = spike_data[neuron_index]\n",
    "#     print(f\"Neuron {neuron_index} spiked at times {spikes}\")\n",
    "    print(len(spikes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83028b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n"
     ]
    }
   ],
   "source": [
    "segment_length = len(train_spikes[0]) // 5 \n",
    "print(segment_length)\n",
    "duration_of_data_point = 50*ms\n",
    "\n",
    "recorded_spikes = []\n",
    "\n",
    "idx = 0\n",
    "for data_point in test_spikes:\n",
    "    \n",
    "    for i in range(num_input_neurons):\n",
    "        start, end = i * segment_length, (i + 1) * segment_length\n",
    "        segment = data_point[start:end]\n",
    "        indices, times = convert_to_spike_times(segment, 5*ms)\n",
    "#         print(times)\n",
    "        \n",
    "        input_neurons[i].set_spikes(indices, times)\n",
    "    \n",
    "    net.run(duration_of_data_point)\n",
    "    net.t_ = 0*ms\n",
    "    recorded_spikes.append(M.spike_trains())\n",
    "\n",
    "\n",
    "    net.remove(M)\n",
    "    M = SpikeMonitor(G)\n",
    "    net.add(M)\n",
    "    \n",
    "    # Reset neuron states\n",
    "    G.v = '0'\n",
    "\n",
    "    for inp_neuron in input_neurons:\n",
    "        inp_neuron.set_spikes([0], [0*ms])\n",
    "        \n",
    "    if idx % 10 == 0:\n",
    "        print(idx)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbac4954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199, 10)\n"
     ]
    }
   ],
   "source": [
    "x_test = recorded_spikes_to_dataset(recorded_spikes, n_out)\n",
    "print(np.shape(x_test))\n",
    "x_test_snn = torch.from_numpy(x_test).clone()\n",
    "torch.save(x_test_snn, 'x_test_snn.pt')\n",
    "X_test_snn = torch.load('x_test_snn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e8faa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.load('train_labels.pt')\n",
    "test_labels = torch.load('test_labels.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "953ca3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(597, 10)\n",
      "MSE: 847.1319913073451\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "X_train = X_train_snn.numpy()\n",
    "y_train = train_labels.numpy()\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(np.shape(X_train))\n",
    "\n",
    "y_pred = model.predict(X_train)\n",
    "mse = mean_squared_error(y_train, y_pred)\n",
    "print(\"train MSE:\", mse)\n",
    "\n",
    "# print(y_pred[500:550])\n",
    "# print(y_train[500:550])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daacdf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97d4d9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test MSE: 782.5939669647572\n",
      "test MSE 1: 895.8751509341905\n",
      "test MSE 2: 728.6785234649035\n",
      "test MSE 3: 840.5240212540359\n",
      "test MSE 4: 642.9507739917561\n"
     ]
    }
   ],
   "source": [
    "y_test = test_labels.numpy()\n",
    "y_pred_test = model.predict(X_test_snn)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "print(\"test MSE:\", mse)\n",
    "mse = mean_squared_error(y_test[0:49], y_pred_test[0:49])\n",
    "print(\"test MSE 1:\", mse)\n",
    "mse = mean_squared_error(y_test[50:99], y_pred_test[50:99])\n",
    "print(\"test MSE 2:\", mse)\n",
    "mse = mean_squared_error(y_test[100:149], y_pred_test[100:149])\n",
    "print(\"test MSE 3:\", mse)\n",
    "mse = mean_squared_error(y_test[150:198], y_pred_test[150:198])\n",
    "print(\"test MSE 4:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c9bcc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test = torch.load('raw_test_seqs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ee2cf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(149.8412)\n"
     ]
    }
   ],
   "source": [
    "print(raw_test[2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03b504e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test_obs = []\n",
    "for i in range(raw_test.shape[0]):\n",
    "    raw_test_obs.append(raw_test[i, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10c1f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance(lst):\n",
    "    mean = np.sum(lst) / len(lst)\n",
    "    sum_of_squares = np.sum((x - mean) ** 2 for x in lst)\n",
    "    var = sum_of_squares / len(lst)\n",
    "\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "289cbee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    /var/folders/nx/bpsgh7x551bb7hrh2sy1khhw0000gn/T/ipykernel_75066/1542879893.py:6: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  sum_of_squares = np.sum((x - mean) ** 2 for x in lst)\n",
      " [py.warnings]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(890.2731)\n",
      "tensor(625.0175)\n",
      "tensor(699.3123)\n",
      "tensor(512.1569)\n"
     ]
    }
   ],
   "source": [
    "# compute variance\n",
    "v1 = calculate_variance(raw_test_obs[0:49])\n",
    "v2 = calculate_variance(raw_test_obs[50:99])\n",
    "v3 = calculate_variance(raw_test_obs[100:149])\n",
    "v4 = calculate_variance(raw_test_obs[150:198])\n",
    "print(v1)\n",
    "print(v2)\n",
    "print(v3)\n",
    "print(v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff37ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601223b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
